{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model Library\n",
    "\n",
    "Compiles list of 587 different models total, varying classifier type and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_models(model, param_grid):\n",
    "    print('Building %s models' % str(model).split('.')[-1][:-2]) \n",
    "    \n",
    "    return list(model(**params) for params in ParameterGrid(param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def xgb_models():\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200], \n",
    "        'objective': ['binary:logistic'],   \n",
    "        'max_depth': [3,5,7,10], \n",
    "        'colsample_bytree': [0.5, 0.8, 0.9],\n",
    "        'subsample': [0.5, 0.8, 0.9]\n",
    "    }\n",
    "    \n",
    "    \n",
    "    return build_models(XGBClassifier, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rf_models():\n",
    "    param_grid = {\n",
    "        'n_estimators' : [20, 50, 100],\n",
    "        'criterion':  ['gini', 'entropy'],\n",
    "        'max_features': [None, 'auto', 'sqrt', 'log2'],\n",
    "        'max_depth': [3, 5, 10, 20, 25] \n",
    "    }\n",
    "    \n",
    "    return build_models(RandomForestClassifier, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linsvm_models():\n",
    "    Cs = np.logspace(-7, 2, 10)\n",
    "    \n",
    "    param_grid = {\n",
    "        'C': Cs,\n",
    "        'loss': ['hinge', 'squared_hinge'],\n",
    "    }\n",
    "    \n",
    "    return build_models(LinearSVC, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rbfsvm_models():\n",
    "    Cs = np.logspace(-7, 0, 9)\n",
    "    gammas = np.logspace(-6, 2, 9, base=2)\n",
    "    \n",
    "    param_grid = {\n",
    "        'kernel': ['rbf'],\n",
    "        'C': Cs,\n",
    "        'gamma': gammas\n",
    "    }\n",
    "    \n",
    "    models = []\n",
    "    f_select = SelectKBest(f_classif, k=50)\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        models.append(Pipeline([('filter', f_select), ('svc', SVC(**params))]))\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dt_models():\n",
    "    param_grid = {\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_features': [None, 'auto', 'sqrt', 'log2'],\n",
    "        'max_depth': [None,1,2,5,10],\n",
    "        'random_state': np.random.randint(100, size=3)\n",
    "    }\n",
    "    \n",
    "    return build_models(DecisionTreeClassifier, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_models():\n",
    "    Cs = np.logspace(-4, 4, 5)\n",
    "    \n",
    "    param_grid = {\n",
    "        'C': Cs\n",
    "    }\n",
    "    \n",
    "    return build_models(LogisticRegression, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def knn_models():\n",
    "    param_grid = {\n",
    "        'n_neighbors': np.linspace(1, 300, num=25, dtype='int')\n",
    "    }\n",
    "    \n",
    "    return build_models(KNeighborsClassifier, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd_models():\n",
    "    param_grid = {\n",
    "        'loss': ['log', 'modified_huber'],\n",
    "        'penalty': ['elasticnet'],\n",
    "        'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "        'learning_rate': ['constant', 'optimal'],\n",
    "        'l1_ratio': np.linspace(0.0, 1.0, 3),\n",
    "        'eta0': [0.001, 0.01, 0.1]\n",
    "    }\n",
    "    \n",
    "    return build_models(SGDClassifier, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building XGBClassifier models\n",
      "Building RandomForestClassifier models\n",
      "Building LinearSVC models\n",
      "Building DecisionTreeClassifier models\n",
      "Building LogisticRegression models\n",
      "Building KNeighborsClassifier models\n",
      "Building SGDClassifier models\n"
     ]
    }
   ],
   "source": [
    "models_dict = {}\n",
    "models_dict = {\n",
    "    'xgb': xgb_models(),\n",
    "    'rf': rf_models(),\n",
    "    'linsvm': linsvm_models(),\n",
    "    'rbfsvm': rbfsvm_models(),\n",
    "    'dt': dt_models(),\n",
    "    'log': log_models(),\n",
    "    'knn': knn_models(),\n",
    "    'sgd': sgd_models()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Ensemble Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data_utils import load_train, load_test, write_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = load_train('data/train_2008.csv', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train all models on a training set\n",
    "\n",
    "We will store all the trained models in a directory so this only needs to be done once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((58200, 366), (58200,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "for k in models_dict:\n",
    "    i = 0\n",
    "    for m in models_dict[k]:\n",
    "        models.append(('%s_%d' %(k, i), m))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "587"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xgb_0'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[371][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally attempted a parallelized approach (see ensemble.py), but my computer couldn't take it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Started at 371 because I had previously trained models and interrupted process midway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb_0 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.5)\n",
      "\n",
      "xgb_1 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8)\n",
      "\n",
      "xgb_2 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.9)\n",
      "\n",
      "xgb_3 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.5)\n",
      "\n",
      "xgb_4 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8)\n",
      "\n",
      "xgb_5 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.9)\n",
      "\n",
      "xgb_6 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.5)\n",
      "\n",
      "xgb_7 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8)\n",
      "\n",
      "xgb_8 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.9)\n",
      "\n",
      "xgb_9 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.5)\n",
      "\n",
      "xgb_10 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8)\n",
      "\n",
      "xgb_11 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.9)\n",
      "\n",
      "xgb_12 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.5)\n",
      "\n",
      "xgb_13 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8)\n",
      "\n",
      "xgb_14 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.9)\n",
      "\n",
      "xgb_15 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.5)\n",
      "\n",
      "xgb_16 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8)\n",
      "\n",
      "xgb_17 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.9)\n",
      "\n",
      "xgb_18 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.5)\n",
      "\n",
      "xgb_19 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8)\n",
      "\n",
      "xgb_20 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.9)\n",
      "\n",
      "xgb_21 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.5)\n",
      "\n",
      "xgb_22 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8)\n",
      "\n",
      "xgb_23 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.9)\n",
      "\n",
      "xgb_24 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.5)\n",
      "\n",
      "xgb_25 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8)\n",
      "\n",
      "xgb_26 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.9)\n",
      "\n",
      "xgb_27 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.5)\n",
      "\n",
      "xgb_28 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8)\n",
      "\n",
      "xgb_29 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.9)\n",
      "\n",
      "xgb_30 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.5)\n",
      "\n",
      "xgb_31 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8)\n",
      "\n",
      "xgb_32 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.9)\n",
      "\n",
      "xgb_33 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.5)\n",
      "\n",
      "xgb_34 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8)\n",
      "\n",
      "xgb_35 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.9)\n",
      "\n",
      "xgb_36 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.5)\n",
      "\n",
      "xgb_37 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8)\n",
      "\n",
      "xgb_38 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.9)\n",
      "\n",
      "xgb_39 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.5)\n",
      "\n",
      "xgb_40 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8)\n",
      "\n",
      "xgb_41 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.9)\n",
      "\n",
      "xgb_42 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.5)\n",
      "\n",
      "xgb_43 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8)\n",
      "\n",
      "xgb_44 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.9)\n",
      "\n",
      "xgb_45 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.5)\n",
      "\n",
      "xgb_46 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8)\n",
      "\n",
      "xgb_47 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.9)\n",
      "\n",
      "xgb_48 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.5)\n",
      "\n",
      "xgb_49 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8)\n",
      "\n",
      "xgb_50 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.9)\n",
      "\n",
      "xgb_51 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.5)\n",
      "\n",
      "xgb_52 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8)\n",
      "\n",
      "xgb_53 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.9)\n",
      "\n",
      "xgb_54 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.5)\n",
      "\n",
      "xgb_55 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8)\n",
      "\n",
      "xgb_56 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.9)\n",
      "\n",
      "xgb_57 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.5)\n",
      "\n",
      "xgb_58 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8)\n",
      "\n",
      "xgb_59 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.9)\n",
      "\n",
      "xgb_60 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.5)\n",
      "\n",
      "xgb_61 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8)\n",
      "\n",
      "xgb_62 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.9)\n",
      "\n",
      "xgb_63 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.5)\n",
      "\n",
      "xgb_64 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8)\n",
      "\n",
      "xgb_65 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.9)\n",
      "\n",
      "xgb_66 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.5)\n",
      "\n",
      "xgb_67 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8)\n",
      "\n",
      "xgb_68 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.9)\n",
      "\n",
      "xgb_69 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.5)\n",
      "\n",
      "xgb_70 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8)\n",
      "\n",
      "xgb_71 - XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.9)\n",
      "\n",
      "sgd_0 - SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_1 - SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_2 - SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_3 - SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_4 - SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_5 - SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_6 - SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_7 - SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_8 - SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_9 - SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_10 - SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_11 - SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_12 - SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_13 - SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_14 - SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_15 - SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_16 - SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_17 - SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_18 - SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_19 - SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_20 - SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_21 - SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_22 - SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_23 - SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_24 - SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_25 - SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_26 - SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=0.0, learning_rate='optimal',\n",
      "       loss='log', n_iter=5, n_jobs=1, penalty='elasticnet', power_t=0.5,\n",
      "       random_state=None, shuffle=True, verbose=0, warm_start=False)\n",
      "\n",
      "sgd_27 - SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=0.0, learning_rate='optimal',\n",
      "       loss='modified_huber', n_iter=5, n_jobs=1, penalty='elasticnet',\n",
      "       power_t=0.5, random_state=None, shuffle=True, verbose=0,\n",
      "       warm_start=False)\n",
      "\n",
      "sgd_28 - SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_29 - SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_30 - SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=0.5, learning_rate='optimal',\n",
      "       loss='log', n_iter=5, n_jobs=1, penalty='elasticnet', power_t=0.5,\n",
      "       random_state=None, shuffle=True, verbose=0, warm_start=False)\n",
      "\n",
      "sgd_31 - SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=0.5, learning_rate='optimal',\n",
      "       loss='modified_huber', n_iter=5, n_jobs=1, penalty='elasticnet',\n",
      "       power_t=0.5, random_state=None, shuffle=True, verbose=0,\n",
      "       warm_start=False)\n",
      "\n",
      "sgd_32 - SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_33 - SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_34 - SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=1.0, learning_rate='optimal',\n",
      "       loss='log', n_iter=5, n_jobs=1, penalty='elasticnet', power_t=0.5,\n",
      "       random_state=None, shuffle=True, verbose=0, warm_start=False)\n",
      "\n",
      "sgd_35 - SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=1.0, learning_rate='optimal',\n",
      "       loss='modified_huber', n_iter=5, n_jobs=1, penalty='elasticnet',\n",
      "       power_t=0.5, random_state=None, shuffle=True, verbose=0,\n",
      "       warm_start=False)\n",
      "\n",
      "sgd_36 - SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_37 - SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_38 - SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_39 - SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_40 - SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_41 - SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_42 - SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_43 - SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_44 - SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_45 - SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_46 - SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_47 - SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_48 - SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_49 - SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_50 - SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_51 - SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_52 - SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_53 - SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_54 - SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_55 - SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_56 - SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_57 - SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_58 - SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_59 - SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_60 - SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_61 - SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_62 - SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=0.0, learning_rate='optimal',\n",
      "       loss='log', n_iter=5, n_jobs=1, penalty='elasticnet', power_t=0.5,\n",
      "       random_state=None, shuffle=True, verbose=0, warm_start=False)\n",
      "\n",
      "sgd_63 - SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=0.0, learning_rate='optimal',\n",
      "       loss='modified_huber', n_iter=5, n_jobs=1, penalty='elasticnet',\n",
      "       power_t=0.5, random_state=None, shuffle=True, verbose=0,\n",
      "       warm_start=False)\n",
      "\n",
      "sgd_64 - SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_65 - SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_66 - SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=0.5, learning_rate='optimal',\n",
      "       loss='log', n_iter=5, n_jobs=1, penalty='elasticnet', power_t=0.5,\n",
      "       random_state=None, shuffle=True, verbose=0, warm_start=False)\n",
      "\n",
      "sgd_67 - SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=0.5, learning_rate='optimal',\n",
      "       loss='modified_huber', n_iter=5, n_jobs=1, penalty='elasticnet',\n",
      "       power_t=0.5, random_state=None, shuffle=True, verbose=0,\n",
      "       warm_start=False)\n",
      "\n",
      "sgd_68 - SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_69 - SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_70 - SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=1.0, learning_rate='optimal',\n",
      "       loss='log', n_iter=5, n_jobs=1, penalty='elasticnet', power_t=0.5,\n",
      "       random_state=None, shuffle=True, verbose=0, warm_start=False)\n",
      "\n",
      "sgd_71 - SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=1.0, learning_rate='optimal',\n",
      "       loss='modified_huber', n_iter=5, n_jobs=1, penalty='elasticnet',\n",
      "       power_t=0.5, random_state=None, shuffle=True, verbose=0,\n",
      "       warm_start=False)\n",
      "\n",
      "sgd_72 - SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_73 - SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_74 - SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_75 - SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_76 - SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_77 - SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_78 - SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_79 - SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_80 - SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_81 - SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_82 - SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_83 - SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_84 - SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_85 - SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_86 - SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_87 - SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_88 - SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_89 - SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_90 - SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_91 - SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_92 - SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_93 - SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_94 - SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_95 - SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_96 - SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_97 - SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_98 - SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=0.0, learning_rate='optimal',\n",
      "       loss='log', n_iter=5, n_jobs=1, penalty='elasticnet', power_t=0.5,\n",
      "       random_state=None, shuffle=True, verbose=0, warm_start=False)\n",
      "\n",
      "sgd_99 - SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=0.0, learning_rate='optimal',\n",
      "       loss='modified_huber', n_iter=5, n_jobs=1, penalty='elasticnet',\n",
      "       power_t=0.5, random_state=None, shuffle=True, verbose=0,\n",
      "       warm_start=False)\n",
      "\n",
      "sgd_100 - SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_101 - SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_102 - SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=0.5, learning_rate='optimal',\n",
      "       loss='log', n_iter=5, n_jobs=1, penalty='elasticnet', power_t=0.5,\n",
      "       random_state=None, shuffle=True, verbose=0, warm_start=False)\n",
      "\n",
      "sgd_103 - SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=0.5, learning_rate='optimal',\n",
      "       loss='modified_huber', n_iter=5, n_jobs=1, penalty='elasticnet',\n",
      "       power_t=0.5, random_state=None, shuffle=True, verbose=0,\n",
      "       warm_start=False)\n",
      "\n",
      "sgd_104 - SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_105 - SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_106 - SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=1.0, learning_rate='optimal',\n",
      "       loss='log', n_iter=5, n_jobs=1, penalty='elasticnet', power_t=0.5,\n",
      "       random_state=None, shuffle=True, verbose=0, warm_start=False)\n",
      "\n",
      "sgd_107 - SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=1.0, learning_rate='optimal',\n",
      "       loss='modified_huber', n_iter=5, n_jobs=1, penalty='elasticnet',\n",
      "       power_t=0.5, random_state=None, shuffle=True, verbose=0,\n",
      "       warm_start=False)\n",
      "\n",
      "sgd_108 - SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_109 - SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_110 - SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_111 - SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_112 - SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_113 - SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_114 - SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_115 - SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_116 - SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_117 - SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_118 - SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_119 - SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_120 - SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_121 - SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_122 - SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_123 - SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_124 - SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_125 - SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_126 - SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_127 - SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_128 - SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_129 - SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_130 - SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_131 - SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_132 - SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_133 - SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_134 - SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=0.0, learning_rate='optimal',\n",
      "       loss='log', n_iter=5, n_jobs=1, penalty='elasticnet', power_t=0.5,\n",
      "       random_state=None, shuffle=True, verbose=0, warm_start=False)\n",
      "\n",
      "sgd_135 - SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=0.0, learning_rate='optimal',\n",
      "       loss='modified_huber', n_iter=5, n_jobs=1, penalty='elasticnet',\n",
      "       power_t=0.5, random_state=None, shuffle=True, verbose=0,\n",
      "       warm_start=False)\n",
      "\n",
      "sgd_136 - SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_137 - SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_138 - SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=0.5, learning_rate='optimal',\n",
      "       loss='log', n_iter=5, n_jobs=1, penalty='elasticnet', power_t=0.5,\n",
      "       random_state=None, shuffle=True, verbose=0, warm_start=False)\n",
      "\n",
      "sgd_139 - SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=0.5, learning_rate='optimal',\n",
      "       loss='modified_huber', n_iter=5, n_jobs=1, penalty='elasticnet',\n",
      "       power_t=0.5, random_state=None, shuffle=True, verbose=0,\n",
      "       warm_start=False)\n",
      "\n",
      "sgd_140 - SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_141 - SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "sgd_142 - SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=1.0, learning_rate='optimal',\n",
      "       loss='log', n_iter=5, n_jobs=1, penalty='elasticnet', power_t=0.5,\n",
      "       random_state=None, shuffle=True, verbose=0, warm_start=False)\n",
      "\n",
      "sgd_143 - SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=1.0, learning_rate='optimal',\n",
      "       loss='modified_huber', n_iter=5, n_jobs=1, penalty='elasticnet',\n",
      "       power_t=0.5, random_state=None, shuffle=True, verbose=0,\n",
      "       warm_start=False)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SAVE_DIR = 'models/ensemble_models'\n",
    "\n",
    "for model in models[371:]:\n",
    "    model[1].fit(X_train, y_train)\n",
    "\n",
    "    print '%s - %s\\n' %(model[0], model[1])\n",
    "    joblib.dump(model[1], os.path.join(SAVE_DIR, '%s.pkl'%(model[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Ensemble Classifier\n",
    "Used log probabilities as a metric for scoring the ensemble. Encapsulated everything into a EnsembleClassifier class later (see ensemble.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all the trained models from directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "list_m = glob.glob(\"models/ensemble_models/*.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models = [joblib.load(m) for m in list_m]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "578"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use hillclimbing algorithm to select a single model each time, based on the log loss of the ensemble model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selected_models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-267-ed614c01f1e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mnew_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mnew_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/neighbors/classification.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mneigh_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/neighbors/base.pyc\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 delayed(self._tree.query, check_pickle=False)(\n\u001b[1;32m    380\u001b[0m                     X[s], n_neighbors, return_distance)\n\u001b[0;32m--> 381\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m             )\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "classes = np.array([])\n",
    "while len(selected_models) < 100:\n",
    "    best_loss = 1e6\n",
    "    best_h = None                                      \n",
    "    \n",
    "    for i in xrange(len(models)):\n",
    "        if ((i + 1) % 25 == 0):\n",
    "            print 'Checking Model: %d/%d' %(i+1, len(models))\n",
    "            print 'Current Loss: %f' %(best_loss)\n",
    "        \n",
    "        if len(classes) == 0:\n",
    "            new_classes = np.asarray([models[i].predict(X_val)])\n",
    "        else:\n",
    "            new_classes = np.vstack([classes, models[i].predict(X_val)])\n",
    "        \n",
    "        n = new_classes.shape[0]\n",
    "        conf = np.sum(y_val == new_classes, axis=0)/ float(n)\n",
    "        log_probs = -1 * np.log(conf.clip(min=1e-6))\n",
    "        loss = np.mean(log_probs)\n",
    "        \n",
    "        # Checks if model produces a better score                                                     \n",
    "        if (loss < best_loss):\n",
    "            best_loss = loss\n",
    "            best_h = models[i]\n",
    "        \n",
    "        \n",
    "    print 'Adding to Ensemble: %s' %(best_h)\n",
    "    print 'Loss: %f' %(best_loss)\n",
    "\n",
    "    selected_models.append(best_h)\n",
    "    if len(classes) == 0:\n",
    "        classes = np.asarray([best_h.predict(X_val)])\n",
    "    else:\n",
    "        classes = np.vstack([classes, best_h.predict(X_val)])\n",
    "    \n",
    "    y_pred = np.asarray([np.argmax(np.bincount(classes[:,c])) for c in range(classes.shape[1])])\n",
    "    acc = accuracy_score(y_pred, y_val)\n",
    "    print 'Accuracy: %f' %(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should have shuffled the models each time, since it looks like same models are being chosen. Changed metric to just pure accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Model: 25/578\n",
      "Current Acc: 0.936137\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.971857\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.971857\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.971857\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.971857\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.971857\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.971857\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.971857\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.971857\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.971857\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.971857\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.971857\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.971857\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.971857\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.971857\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.971857\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.971857\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.971857\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.971857\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.971857\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.971857\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.971857\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.971857\n",
      "Adding to Ensemble: SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "Acc: 0.971857\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.941704\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.976341\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.976341\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.976341\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.976341\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.976341\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.976341\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.976341\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.976341\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.976341\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.976341\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.976341\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.976341\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.976341\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.976341\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.976341\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.976341\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.976341\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.976341\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.976341\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.976341\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.976341\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.976341\n",
      "Adding to Ensemble: SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "Acc: 0.976341\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.983300\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.983454\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.984537\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.985465\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.985465\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.985465\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.985465\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.985465\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.985465\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.985465\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.985465\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.985465\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.985465\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.985465\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.985465\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.985465\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.985465\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.985465\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.985465\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.985465\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.985465\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.985465\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.985465\n",
      "Adding to Ensemble: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=20, max_features='log2', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=20, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "Acc: 0.985465\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.981754\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.981754\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.985310\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.985465\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.985465\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.985465\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.985465\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.985465\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.985465\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.985465\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.985465\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.985465\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.985465\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.985465\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.985465\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.985465\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.985465\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.985465\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.985465\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.985465\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.985465\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.985465\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.985465\n",
      "Adding to Ensemble: SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "Acc: 0.985465\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.991650\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.991650\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.991650\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.991650\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.993196\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.993196\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.993196\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.993196\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.993815\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.993815\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.993815\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.993815\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.993815\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.993815\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.993815\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.993815\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.993815\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.993815\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.993815\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.993815\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.993815\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.993815\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.993815\n",
      "Adding to Ensemble: DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=5,\n",
      "            max_features='log2', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=97, splitter='best')\n",
      "Acc: 0.993815\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.989949\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.989949\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.993660\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.993815\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.993815\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.993815\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.993815\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.993815\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.993815\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.993815\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.993815\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.993815\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.993815\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.993815\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.993815\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.993815\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.993815\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.993815\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.993815\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.993815\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.993815\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.993815\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.993815\n",
      "Adding to Ensemble: SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "Acc: 0.993815\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.993815\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.993815\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.993815\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.993815\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.993969\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.993969\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.993969\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.993969\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.994124\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.994124\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.994124\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.994124\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.994124\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.994124\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.994124\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.994124\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.994124\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.994124\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.994124\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.994124\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.994124\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.994124\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.994124\n",
      "Adding to Ensemble: XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
      "       min_child_weight=1, missing=nan, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.5)\n",
      "Acc: 0.994124\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.991805\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.994588\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.995825\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.995825\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.995825\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.995825\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.995825\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.995825\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.995825\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.995825\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.995825\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.995825\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.995825\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.995825\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.995825\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.995825\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.995825\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.995825\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.995825\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.995825\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.995825\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.995825\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.995825\n",
      "Adding to Ensemble: Pipeline(steps=[('filter', SelectKBest(k=50, score_func=<function f_classif at 0x109fe80c8>)), ('svc', SVC(C=1.0000000000000001e-05, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.0625, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "Acc: 0.995825\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.995980\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.996134\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.996134\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.996443\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.996443\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.996443\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.996443\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.996443\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.996443\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.996443\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.996443\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.996443\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.996443\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.996443\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.996443\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.996443\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.996443\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.996443\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.996443\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.996443\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.996443\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.996443\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.996443\n",
      "Adding to Ensemble: XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
      "       min_child_weight=1, missing=nan, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.9)\n",
      "Acc: 0.996443\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.993660\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.996443\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.996443\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.996598\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.996598\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.996598\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.996598\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.996753\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.996753\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.996753\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.996753\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.996753\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.996753\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.996753\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.996753\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.996753\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.996753\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.996753\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.996753\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.996753\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.996753\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.996753\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.996753\n",
      "Adding to Ensemble: DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=1,\n",
      "            max_features='log2', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=31, splitter='best')\n",
      "Acc: 0.996753\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.996907\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.997062\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.997062\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.997217\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.997217\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.997217\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.997217\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.997217\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.997217\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.997217\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.997217\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.997217\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.997217\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.997371\n",
      "Adding to Ensemble: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=20, max_features='log2', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "Acc: 0.997371\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.994433\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.996907\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.997217\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.997371\n",
      "Adding to Ensemble: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
      "            max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=97, splitter='best')\n",
      "Acc: 0.997371\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.997371\n",
      "Adding to Ensemble: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=5, max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "Acc: 0.997371\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.994433\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.996907\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.997217\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.997371\n",
      "Adding to Ensemble: XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
      "       min_child_weight=1, missing=nan, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.9)\n",
      "Acc: 0.997371\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.997371\n",
      "Adding to Ensemble: SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "Acc: 0.997371\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.994433\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.996907\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.997217\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.997371\n",
      "Adding to Ensemble: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=113, p=2,\n",
      "           weights='uniform')\n",
      "Acc: 0.997371\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.997371\n",
      "Adding to Ensemble: Pipeline(steps=[('filter', SelectKBest(k=50, score_func=<function f_classif at 0x109fe80c8>)), ('svc', SVC(C=0.0001, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.015625, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "Acc: 0.997371\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.994433\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.996907\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.997217\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.997371\n",
      "Adding to Ensemble: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=63, p=2,\n",
      "           weights='uniform')\n",
      "Acc: 0.997371\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.997371\n",
      "Adding to Ensemble: Pipeline(steps=[('filter', SelectKBest(k=50, score_func=<function f_classif at 0x109fe80c8>)), ('svc', SVC(C=0.0001, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=4.0, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "Acc: 0.997371\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.994433\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.996907\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.997217\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.997371\n",
      "Adding to Ensemble: DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=31, splitter='best')\n",
      "Acc: 0.997371\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.997371\n",
      "Adding to Ensemble: Pipeline(steps=[('filter', SelectKBest(k=50, score_func=<function f_classif at 0x109fe80c8>)), ('svc', SVC(C=9.9999999999999995e-08, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.5, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "Acc: 0.997371\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.994433\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.996907\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.997217\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.997371\n",
      "Adding to Ensemble: SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "Acc: 0.997371\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.997371\n",
      "Adding to Ensemble: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=31, splitter='best')\n",
      "Acc: 0.997371\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.994433\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.996907\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.997217\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.997371\n",
      "Adding to Ensemble: SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "Acc: 0.997371\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.997371\n",
      "Adding to Ensemble: Pipeline(steps=[('filter', SelectKBest(k=50, score_func=<function f_classif at 0x109fe80c8>)), ('svc', SVC(C=9.9999999999999995e-07, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.03125, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "Acc: 0.997371\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.994433\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.996907\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.997217\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.997371\n",
      "Adding to Ensemble: XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=nan, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8)\n",
      "Acc: 0.997371\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.997371\n",
      "Adding to Ensemble: XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
      "       min_child_weight=1, missing=nan, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.9)\n",
      "Acc: 0.997371\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.994433\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.996907\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.997217\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.997371\n",
      "Adding to Ensemble: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=97, splitter='best')\n",
      "Acc: 0.997371\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.997371\n",
      "Adding to Ensemble: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
      "            max_features='log2', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=17, splitter='best')\n",
      "Acc: 0.997371\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.994433\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.996907\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.997217\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.997371\n",
      "Adding to Ensemble: XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=nan, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8)\n",
      "Acc: 0.997371\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.997371\n",
      "Adding to Ensemble: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=212, p=2,\n",
      "           weights='uniform')\n",
      "Acc: 0.997371\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.994433\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.996907\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.997217\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.997371\n",
      "Adding to Ensemble: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=17, splitter='best')\n",
      "Acc: 0.997371\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.997371\n",
      "Adding to Ensemble: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features='log2', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=17, splitter='best')\n",
      "Acc: 0.997371\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.994433\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.996907\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.997217\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.997371\n",
      "Adding to Ensemble: LinearSVC(C=9.9999999999999995e-07, class_weight=None, dual=True,\n",
      "     fit_intercept=True, intercept_scaling=1, loss='hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "Acc: 0.997371\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.997371\n",
      "Adding to Ensemble: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=97, splitter='best')\n",
      "Acc: 0.997371\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.994433\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.996907\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.997217\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.997371\n",
      "Adding to Ensemble: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=17, splitter='best')\n",
      "Acc: 0.997371\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.997371\n",
      "Adding to Ensemble: SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "Acc: 0.997371\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.994433\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.996907\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.997217\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.997371\n",
      "Adding to Ensemble: SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "Acc: 0.997371\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.997371\n",
      "Adding to Ensemble: XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
      "       min_child_weight=1, missing=nan, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.9)\n",
      "Acc: 0.997371\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.994433\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.996907\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.997217\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.997371\n",
      "Adding to Ensemble: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=162, p=2,\n",
      "           weights='uniform')\n",
      "Acc: 0.997371\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.997371\n",
      "Adding to Ensemble: DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=1,\n",
      "            max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=17, splitter='best')\n",
      "Acc: 0.997371\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.994433\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.996907\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.997217\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.997371\n",
      "Adding to Ensemble: Pipeline(steps=[('filter', SelectKBest(k=50, score_func=<function f_classif at 0x109fe80c8>)), ('svc', SVC(C=0.001, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.03125, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "Acc: 0.997371\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.997371\n",
      "Adding to Ensemble: SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "Acc: 0.997371\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.994433\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.996907\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.997217\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.997371\n",
      "Adding to Ensemble: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
      "            max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=97, splitter='best')\n",
      "Acc: 0.997371\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.997371\n",
      "Adding to Ensemble: DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=10,\n",
      "            max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=31, splitter='best')\n",
      "Acc: 0.997371\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.994433\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.996907\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.997217\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.997371\n",
      "Adding to Ensemble: Pipeline(steps=[('filter', SelectKBest(k=50, score_func=<function f_classif at 0x109fe80c8>)), ('svc', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.125, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "Acc: 0.997371\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.997371\n",
      "Adding to Ensemble: SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "Acc: 0.997371\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.994433\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.996907\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.997217\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.997371\n",
      "Adding to Ensemble: SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "Acc: 0.997371\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.997371\n",
      "Adding to Ensemble: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=38, p=2,\n",
      "           weights='uniform')\n",
      "Acc: 0.997371\n",
      "Checking Model: 25/578\n",
      "Current Acc: 0.994433\n",
      "Checking Model: 50/578\n",
      "Current Acc: 0.996907\n",
      "Checking Model: 75/578\n",
      "Current Acc: 0.997217\n",
      "Checking Model: 100/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 125/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 150/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 175/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 200/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 225/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 250/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 275/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 300/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 325/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 350/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 375/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 400/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 425/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 450/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 475/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 500/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 525/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 550/578\n",
      "Current Acc: 0.997371\n",
      "Checking Model: 575/578\n",
      "Current Acc: 0.997371\n",
      "Adding to Ensemble: XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
      "       min_child_weight=1, missing=nan, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.9)\n",
      "Acc: 0.997371\n"
     ]
    }
   ],
   "source": [
    "selected_models = []\n",
    "predictions = np.asarray([clf.predict(X_val) for clf in models])\n",
    "classes = np.array([])\n",
    "\n",
    "while len(selected_models) < 50:\n",
    "    best_acc = 0.0\n",
    "    best_h = None  \n",
    "    best_pred = []\n",
    "    \n",
    "    np.random.shuffle(models)\n",
    "    for i in xrange(len(models)):\n",
    "        if ((i + 1) % 25 == 0):\n",
    "            print 'Checking Model: %d/%d' %(i+1, len(models))\n",
    "            print 'Current Acc: %f' %(best_acc)\n",
    "        \n",
    "        if len(classes) == 0:\n",
    "            new_classes = np.asarray([predictions[i]])\n",
    "        else:\n",
    "            new_classes = np.vstack([classes, predictions[i]])\n",
    "        \n",
    "        y_pred = np.asarray([np.argmax(np.bincount(new_classes[:,c])) for c in range(new_classes.shape[1])])\n",
    "        acc = accuracy_score(y_pred, y_val)\n",
    "        \n",
    "        # Checks if model produces a better score                                                     \n",
    "        if (acc > best_acc):\n",
    "            best_acc = acc\n",
    "            best_h = models[i]\n",
    "            best_pred = predictions[i]\n",
    "        \n",
    "        \n",
    "    print 'Adding to Ensemble: %s' %(best_h)\n",
    "    print 'Acc: %f' %(best_acc)\n",
    "\n",
    "    selected_models.append(best_h)\n",
    "    if len(classes) == 0:\n",
    "        classes = np.asarray([best_pred])\n",
    "    else:\n",
    "        classes = np.vstack([classes, best_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
       "                     max_features='sqrt', max_leaf_nodes=None,\n",
       "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                     presort=False, random_state=17, splitter='best'): 1,\n",
       "         DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
       "                     max_features='sqrt', max_leaf_nodes=None,\n",
       "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                     presort=False, random_state=97, splitter='best'): 1,\n",
       "         DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
       "                     max_features='auto', max_leaf_nodes=None,\n",
       "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                     presort=False, random_state=97, splitter='best'): 1,\n",
       "         DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
       "                     max_features='log2', max_leaf_nodes=None,\n",
       "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                     presort=False, random_state=17, splitter='best'): 1,\n",
       "         DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
       "                     max_features=None, max_leaf_nodes=None,\n",
       "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                     presort=False, random_state=97, splitter='best'): 1,\n",
       "         DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=1,\n",
       "                     max_features='log2', max_leaf_nodes=None,\n",
       "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                     presort=False, random_state=31, splitter='best'): 1,\n",
       "         DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                     max_features=None, max_leaf_nodes=None,\n",
       "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                     presort=False, random_state=17, splitter='best'): 1,\n",
       "         DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
       "                     max_features='log2', max_leaf_nodes=None,\n",
       "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                     presort=False, random_state=17, splitter='best'): 1,\n",
       "         DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=5,\n",
       "                     max_features='log2', max_leaf_nodes=None,\n",
       "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                     presort=False, random_state=97, splitter='best'): 1,\n",
       "         DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=10,\n",
       "                     max_features='sqrt', max_leaf_nodes=None,\n",
       "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                     presort=False, random_state=31, splitter='best'): 1,\n",
       "         DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "                     max_features=None, max_leaf_nodes=None,\n",
       "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                     presort=False, random_state=97, splitter='best'): 1,\n",
       "         DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                     max_features='sqrt', max_leaf_nodes=None,\n",
       "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                     presort=False, random_state=31, splitter='best'): 1,\n",
       "         DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=1,\n",
       "                     max_features=None, max_leaf_nodes=None,\n",
       "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                     presort=False, random_state=31, splitter='best'): 1,\n",
       "         DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=1,\n",
       "                     max_features='auto', max_leaf_nodes=None,\n",
       "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                     presort=False, random_state=17, splitter='best'): 1,\n",
       "         KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                    metric_params=None, n_jobs=1, n_neighbors=162, p=2,\n",
       "                    weights='uniform'): 1,\n",
       "         KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                    metric_params=None, n_jobs=1, n_neighbors=212, p=2,\n",
       "                    weights='uniform'): 1,\n",
       "         KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                    metric_params=None, n_jobs=1, n_neighbors=38, p=2,\n",
       "                    weights='uniform'): 1,\n",
       "         KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                    metric_params=None, n_jobs=1, n_neighbors=63, p=2,\n",
       "                    weights='uniform'): 1,\n",
       "         KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                    metric_params=None, n_jobs=1, n_neighbors=113, p=2,\n",
       "                    weights='uniform'): 1,\n",
       "         LinearSVC(C=9.9999999999999995e-07, class_weight=None, dual=True,\n",
       "              fit_intercept=True, intercept_scaling=1, loss='hinge', max_iter=1000,\n",
       "              multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "              verbose=0): 1,\n",
       "         Pipeline(steps=[('filter', SelectKBest(k=50, score_func=<function f_classif at 0x109fe80c8>)), ('svc', SVC(C=9.9999999999999995e-07, cache_size=200, class_weight=None, coef0=0.0,\n",
       "           decision_function_shape=None, degree=3, gamma=0.03125, kernel='rbf',\n",
       "           max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "           tol=0.001, verbose=False))]): 1,\n",
       "         Pipeline(steps=[('filter', SelectKBest(k=50, score_func=<function f_classif at 0x109fe80c8>)), ('svc', SVC(C=1.0000000000000001e-05, cache_size=200, class_weight=None, coef0=0.0,\n",
       "           decision_function_shape=None, degree=3, gamma=0.0625, kernel='rbf',\n",
       "           max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "           tol=0.001, verbose=False))]): 1,\n",
       "         Pipeline(steps=[('filter', SelectKBest(k=50, score_func=<function f_classif at 0x109fe80c8>)), ('svc', SVC(C=0.0001, cache_size=200, class_weight=None, coef0=0.0,\n",
       "           decision_function_shape=None, degree=3, gamma=0.015625, kernel='rbf',\n",
       "           max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "           tol=0.001, verbose=False))]): 1,\n",
       "         Pipeline(steps=[('filter', SelectKBest(k=50, score_func=<function f_classif at 0x109fe80c8>)), ('svc', SVC(C=0.0001, cache_size=200, class_weight=None, coef0=0.0,\n",
       "           decision_function_shape=None, degree=3, gamma=4.0, kernel='rbf',\n",
       "           max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "           tol=0.001, verbose=False))]): 1,\n",
       "         Pipeline(steps=[('filter', SelectKBest(k=50, score_func=<function f_classif at 0x109fe80c8>)), ('svc', SVC(C=0.001, cache_size=200, class_weight=None, coef0=0.0,\n",
       "           decision_function_shape=None, degree=3, gamma=0.03125, kernel='rbf',\n",
       "           max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "           tol=0.001, verbose=False))]): 1,\n",
       "         Pipeline(steps=[('filter', SelectKBest(k=50, score_func=<function f_classif at 0x109fe80c8>)), ('svc', SVC(C=9.9999999999999995e-08, cache_size=200, class_weight=None, coef0=0.0,\n",
       "           decision_function_shape=None, degree=3, gamma=0.5, kernel='rbf',\n",
       "           max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "           tol=0.001, verbose=False))]): 1,\n",
       "         Pipeline(steps=[('filter', SelectKBest(k=50, score_func=<function f_classif at 0x109fe80c8>)), ('svc', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "           decision_function_shape=None, degree=3, gamma=0.125, kernel='rbf',\n",
       "           max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "           tol=0.001, verbose=False))]): 1,\n",
       "         RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "                     max_depth=20, max_features='log2', max_leaf_nodes=None,\n",
       "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                     n_estimators=20, n_jobs=1, oob_score=False, random_state=None,\n",
       "                     verbose=0, warm_start=False): 1,\n",
       "         RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "                     max_depth=20, max_features='log2', max_leaf_nodes=None,\n",
       "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                     n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
       "                     verbose=0, warm_start=False): 1,\n",
       "         RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "                     max_depth=5, max_features=None, max_leaf_nodes=None,\n",
       "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                     n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
       "                     verbose=0, warm_start=False): 1,\n",
       "         SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
       "                eta0=0.1, fit_intercept=True, l1_ratio=0.5,\n",
       "                learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
       "                penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
       "                verbose=0, warm_start=False): 1,\n",
       "         SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "                eta0=0.01, fit_intercept=True, l1_ratio=0.0,\n",
       "                learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
       "                penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
       "                verbose=0, warm_start=False): 1,\n",
       "         SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
       "                eta0=0.1, fit_intercept=True, l1_ratio=0.5,\n",
       "                learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
       "                penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
       "                verbose=0, warm_start=False): 1,\n",
       "         SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
       "                eta0=0.001, fit_intercept=True, l1_ratio=0.0,\n",
       "                learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
       "                penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
       "                verbose=0, warm_start=False): 1,\n",
       "         SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
       "                eta0=0.001, fit_intercept=True, l1_ratio=0.5,\n",
       "                learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
       "                penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
       "                verbose=0, warm_start=False): 1,\n",
       "         SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "                eta0=0.01, fit_intercept=True, l1_ratio=0.5,\n",
       "                learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
       "                penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
       "                verbose=0, warm_start=False): 1,\n",
       "         SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "                eta0=0.001, fit_intercept=True, l1_ratio=0.0,\n",
       "                learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
       "                penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
       "                verbose=0, warm_start=False): 1,\n",
       "         SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
       "                eta0=0.001, fit_intercept=True, l1_ratio=0.5,\n",
       "                learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
       "                penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
       "                verbose=0, warm_start=False): 2,\n",
       "         SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "                eta0=0.001, fit_intercept=True, l1_ratio=1.0,\n",
       "                learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
       "                penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
       "                verbose=0, warm_start=False): 1,\n",
       "         SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
       "                eta0=0.1, fit_intercept=True, l1_ratio=0.0,\n",
       "                learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
       "                penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
       "                verbose=0, warm_start=False): 1,\n",
       "         SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
       "                eta0=0.01, fit_intercept=True, l1_ratio=1.0,\n",
       "                learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
       "                penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
       "                verbose=0, warm_start=False): 1,\n",
       "         XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
       "                gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
       "                min_child_weight=1, missing=nan, n_estimators=200, nthread=-1,\n",
       "                objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "                scale_pos_weight=1, seed=0, silent=True, subsample=0.9): 1,\n",
       "         XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
       "                gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "                min_child_weight=1, missing=nan, n_estimators=100, nthread=-1,\n",
       "                objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "                scale_pos_weight=1, seed=0, silent=True, subsample=0.8): 2,\n",
       "         XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
       "                gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
       "                min_child_weight=1, missing=nan, n_estimators=100, nthread=-1,\n",
       "                objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "                scale_pos_weight=1, seed=0, silent=True, subsample=0.9): 1,\n",
       "         XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
       "                gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
       "                min_child_weight=1, missing=nan, n_estimators=200, nthread=-1,\n",
       "                objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "                scale_pos_weight=1, seed=0, silent=True, subsample=0.9): 1,\n",
       "         XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
       "                gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
       "                min_child_weight=1, missing=nan, n_estimators=100, nthread=-1,\n",
       "                objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "                scale_pos_weight=1, seed=0, silent=True, subsample=0.5): 1,\n",
       "         XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
       "                gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
       "                min_child_weight=1, missing=nan, n_estimators=200, nthread=-1,\n",
       "                objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "                scale_pos_weight=1, seed=0, silent=True, subsample=0.9): 1,\n",
       "         XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
       "                gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
       "                min_child_weight=1, missing=nan, n_estimators=100, nthread=-1,\n",
       "                objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "                scale_pos_weight=1, seed=0, silent=True, subsample=0.9): 1})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(selected_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models (50) selected using accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models_dict_1 = {DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
    "                     max_features='sqrt', max_leaf_nodes=None,\n",
    "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                     presort=False, random_state=17, splitter='best'): 1,\n",
    "         DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
    "                     max_features='sqrt', max_leaf_nodes=None,\n",
    "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                     presort=False, random_state=97, splitter='best'): 1,\n",
    "         DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
    "                     max_features='auto', max_leaf_nodes=None,\n",
    "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                     presort=False, random_state=97, splitter='best'): 1,\n",
    "         DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
    "                     max_features='log2', max_leaf_nodes=None,\n",
    "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                     presort=False, random_state=17, splitter='best'): 1,\n",
    "         DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
    "                     max_features=None, max_leaf_nodes=None,\n",
    "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                     presort=False, random_state=97, splitter='best'): 1,\n",
    "         DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=1,\n",
    "                     max_features='log2', max_leaf_nodes=None,\n",
    "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                     presort=False, random_state=31, splitter='best'): 1,\n",
    "         DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "                     max_features=None, max_leaf_nodes=None,\n",
    "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                     presort=False, random_state=17, splitter='best'): 1,\n",
    "         DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
    "                     max_features='log2', max_leaf_nodes=None,\n",
    "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                     presort=False, random_state=17, splitter='best'): 1,\n",
    "         DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=5,\n",
    "                     max_features='log2', max_leaf_nodes=None,\n",
    "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                     presort=False, random_state=97, splitter='best'): 1,\n",
    "         DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=10,\n",
    "                     max_features='sqrt', max_leaf_nodes=None,\n",
    "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                     presort=False, random_state=31, splitter='best'): 1,\n",
    "         DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
    "                     max_features=None, max_leaf_nodes=None,\n",
    "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                     presort=False, random_state=97, splitter='best'): 1,\n",
    "         DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "                     max_features='sqrt', max_leaf_nodes=None,\n",
    "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                     presort=False, random_state=31, splitter='best'): 1,\n",
    "         DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=1,\n",
    "                     max_features=None, max_leaf_nodes=None,\n",
    "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                     presort=False, random_state=31, splitter='best'): 1,\n",
    "         DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=1,\n",
    "                     max_features='auto', max_leaf_nodes=None,\n",
    "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                     presort=False, random_state=17, splitter='best'): 1,\n",
    "         KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "                    metric_params=None, n_jobs=1, n_neighbors=162, p=2,\n",
    "                    weights='uniform'): 1,\n",
    "         KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "                    metric_params=None, n_jobs=1, n_neighbors=212, p=2,\n",
    "                    weights='uniform'): 1,\n",
    "         KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "                    metric_params=None, n_jobs=1, n_neighbors=38, p=2,\n",
    "                    weights='uniform'): 1,\n",
    "         KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "                    metric_params=None, n_jobs=1, n_neighbors=63, p=2,\n",
    "                    weights='uniform'): 1,\n",
    "         KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "                    metric_params=None, n_jobs=1, n_neighbors=113, p=2,\n",
    "                    weights='uniform'): 1,\n",
    "         LinearSVC(C=9.9999999999999995e-07, class_weight=None, dual=True,\n",
    "              fit_intercept=True, intercept_scaling=1, loss='hinge', max_iter=1000,\n",
    "              multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
    "              verbose=0): 1,\n",
    "         Pipeline(steps=[('filter', SelectKBest(k=50, score_func=f_classif)), ('svc', SVC(C=9.9999999999999995e-07, cache_size=200, class_weight=None, coef0=0.0,\n",
    "           decision_function_shape=None, degree=3, gamma=0.03125, kernel='rbf',\n",
    "           max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "           tol=0.001, verbose=False))]): 1,\n",
    "         Pipeline(steps=[('filter', SelectKBest(k=50, score_func=f_classif)), ('svc', SVC(C=1.0000000000000001e-05, cache_size=200, class_weight=None, coef0=0.0,\n",
    "           decision_function_shape=None, degree=3, gamma=0.0625, kernel='rbf',\n",
    "           max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "           tol=0.001, verbose=False))]): 1,\n",
    "         Pipeline(steps=[('filter', SelectKBest(k=50, score_func=f_classif)), ('svc', SVC(C=0.0001, cache_size=200, class_weight=None, coef0=0.0,\n",
    "           decision_function_shape=None, degree=3, gamma=0.015625, kernel='rbf',\n",
    "           max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "           tol=0.001, verbose=False))]): 1,\n",
    "         Pipeline(steps=[('filter', SelectKBest(k=50, score_func=f_classif)), ('svc', SVC(C=0.0001, cache_size=200, class_weight=None, coef0=0.0,\n",
    "           decision_function_shape=None, degree=3, gamma=4.0, kernel='rbf',\n",
    "           max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "           tol=0.001, verbose=False))]): 1,\n",
    "         Pipeline(steps=[('filter', SelectKBest(k=50, score_func=f_classif)), ('svc', SVC(C=0.001, cache_size=200, class_weight=None, coef0=0.0,\n",
    "           decision_function_shape=None, degree=3, gamma=0.03125, kernel='rbf',\n",
    "           max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "           tol=0.001, verbose=False))]): 1,\n",
    "         Pipeline(steps=[('filter', SelectKBest(k=50, score_func=f_classif)), ('svc', SVC(C=9.9999999999999995e-08, cache_size=200, class_weight=None, coef0=0.0,\n",
    "           decision_function_shape=None, degree=3, gamma=0.5, kernel='rbf',\n",
    "           max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "           tol=0.001, verbose=False))]): 1,\n",
    "         Pipeline(steps=[('filter', SelectKBest(k=50, score_func=f_classif)), ('svc', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "           decision_function_shape=None, degree=3, gamma=0.125, kernel='rbf',\n",
    "           max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "           tol=0.001, verbose=False))]): 1,\n",
    "         RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
    "                     max_depth=20, max_features='log2', max_leaf_nodes=None,\n",
    "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                     n_estimators=20, n_jobs=1, oob_score=False, random_state=None,\n",
    "                     verbose=0, warm_start=False): 1,\n",
    "         RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
    "                     max_depth=20, max_features='log2', max_leaf_nodes=None,\n",
    "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                     n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
    "                     verbose=0, warm_start=False): 1,\n",
    "         RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
    "                     max_depth=5, max_features=None, max_leaf_nodes=None,\n",
    "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                     n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
    "                     verbose=0, warm_start=False): 1,\n",
    "         SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
    "                eta0=0.1, fit_intercept=True, l1_ratio=0.5,\n",
    "                learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
    "                penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
    "                verbose=0, warm_start=False): 1,\n",
    "         SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
    "                eta0=0.01, fit_intercept=True, l1_ratio=0.0,\n",
    "                learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
    "                penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
    "                verbose=0, warm_start=False): 1,\n",
    "         SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
    "                eta0=0.1, fit_intercept=True, l1_ratio=0.5,\n",
    "                learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
    "                penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
    "                verbose=0, warm_start=False): 1,\n",
    "         SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
    "                eta0=0.001, fit_intercept=True, l1_ratio=0.0,\n",
    "                learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
    "                penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
    "                verbose=0, warm_start=False): 1,\n",
    "         SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
    "                eta0=0.001, fit_intercept=True, l1_ratio=0.5,\n",
    "                learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
    "                penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
    "                verbose=0, warm_start=False): 1,\n",
    "         SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
    "                eta0=0.01, fit_intercept=True, l1_ratio=0.5,\n",
    "                learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
    "                penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
    "                verbose=0, warm_start=False): 1,\n",
    "         SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
    "                eta0=0.001, fit_intercept=True, l1_ratio=0.0,\n",
    "                learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
    "                penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
    "                verbose=0, warm_start=False): 1,\n",
    "         SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
    "                eta0=0.001, fit_intercept=True, l1_ratio=0.5,\n",
    "                learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
    "                penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
    "                verbose=0, warm_start=False): 2,\n",
    "         SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
    "                eta0=0.001, fit_intercept=True, l1_ratio=1.0,\n",
    "                learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
    "                penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
    "                verbose=0, warm_start=False): 1,\n",
    "         SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
    "                eta0=0.1, fit_intercept=True, l1_ratio=0.0,\n",
    "                learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
    "                penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
    "                verbose=0, warm_start=False): 1,\n",
    "         SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
    "                eta0=0.01, fit_intercept=True, l1_ratio=1.0,\n",
    "                learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
    "                penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
    "                verbose=0, warm_start=False): 1,\n",
    "         XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
    "                gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
    "                min_child_weight=1, n_estimators=200, nthread=-1,\n",
    "                objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
    "                scale_pos_weight=1, seed=0, silent=True, subsample=0.9): 1,\n",
    "         XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
    "                gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
    "                min_child_weight=1, n_estimators=100, nthread=-1,\n",
    "                objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
    "                scale_pos_weight=1, seed=0, silent=True, subsample=0.8): 2,\n",
    "         XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
    "                gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
    "                min_child_weight=1, n_estimators=100, nthread=-1,\n",
    "                objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
    "                scale_pos_weight=1, seed=0, silent=True, subsample=0.9): 1,\n",
    "         XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
    "                gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
    "                min_child_weight=1, n_estimators=200, nthread=-1,\n",
    "                objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
    "                scale_pos_weight=1, seed=0, silent=True, subsample=0.9): 1,\n",
    "         XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
    "                gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
    "                min_child_weight=1, n_estimators=100, nthread=-1,\n",
    "                objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
    "                scale_pos_weight=1, seed=0, silent=True, subsample=0.5): 1,\n",
    "         XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
    "                gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
    "                min_child_weight=1, n_estimators=200, nthread=-1,\n",
    "                objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
    "                scale_pos_weight=1, seed=0, silent=True, subsample=0.9): 1,\n",
    "         XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
    "                gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
    "                min_child_weight=1, n_estimators=100, nthread=-1,\n",
    "                objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
    "                scale_pos_weight=1, seed=0, silent=True, subsample=0.9): 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding Classifier: Pipeline(steps=[('filter', SelectKBest(k=50, score_func=<function f_classif at 0x109fe80c8>)), ('svc', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.125, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "Adding Classifier: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=20, max_features='log2', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=20, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "Adding Classifier: XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.9)\n",
      "Adding Classifier: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=5, max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "Adding Classifier: SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "Adding Classifier: SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "Adding Classifier: SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "Adding Classifier: XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.9)\n",
      "Adding Classifier: SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "Adding Classifier: SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "Adding Classifier: SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "Adding Classifier: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=20, max_features='log2', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "Adding Classifier: SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=0.5,\n",
      "       learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "Adding Classifier: SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "Adding Classifier: XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.5)\n",
      "Adding Classifier: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=17, splitter='best')\n",
      "Adding Classifier: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
      "            max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=97, splitter='best')\n",
      "Adding Classifier: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
      "            max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=97, splitter='best')\n",
      "Adding Classifier: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features='log2', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=17, splitter='best')\n",
      "Adding Classifier: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=97, splitter='best')\n",
      "Adding Classifier: DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=1,\n",
      "            max_features='log2', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=31, splitter='best')\n",
      "Adding Classifier: XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.9)\n",
      "Adding Classifier: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=17, splitter='best')\n",
      "Adding Classifier: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
      "            max_features='log2', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=17, splitter='best')\n",
      "Adding Classifier: DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=5,\n",
      "            max_features='log2', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=97, splitter='best')\n",
      "Adding Classifier: DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=10,\n",
      "            max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=31, splitter='best')\n",
      "Adding Classifier: SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.001, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "Adding Classifier: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=97, splitter='best')\n",
      "Adding Classifier: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=31, splitter='best')\n",
      "Adding Classifier: XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.9)\n",
      "Adding Classifier: DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=31, splitter='best')\n",
      "Adding Classifier: DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=1,\n",
      "            max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=17, splitter='best')\n",
      "Adding Classifier: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=162, p=2,\n",
      "           weights='uniform')\n",
      "Adding Classifier: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=212, p=2,\n",
      "           weights='uniform')\n",
      "Adding Classifier: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=38, p=2,\n",
      "           weights='uniform')\n",
      "Adding Classifier: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=63, p=2,\n",
      "           weights='uniform')\n",
      "Adding Classifier: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=113, p=2,\n",
      "           weights='uniform')\n",
      "Adding Classifier: LinearSVC(C=1e-06, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
      "     penalty='l2', random_state=None, tol=0.0001, verbose=0)\n",
      "Adding Classifier: Pipeline(steps=[('filter', SelectKBest(k=50, score_func=<function f_classif at 0x109fe80c8>)), ('svc', SVC(C=1e-06, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.03125, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "Adding Classifier: SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.1, fit_intercept=True, l1_ratio=0.0,\n",
      "       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "Adding Classifier: Pipeline(steps=[('filter', SelectKBest(k=50, score_func=<function f_classif at 0x109fe80c8>)), ('svc', SVC(C=1e-05, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.0625, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "Adding Classifier: SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=1.0,\n",
      "       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "Adding Classifier: Pipeline(steps=[('filter', SelectKBest(k=50, score_func=<function f_classif at 0x109fe80c8>)), ('svc', SVC(C=0.0001, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.015625, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "Adding Classifier: Pipeline(steps=[('filter', SelectKBest(k=50, score_func=<function f_classif at 0x109fe80c8>)), ('svc', SVC(C=0.0001, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=4.0, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "Adding Classifier: XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.9)\n",
      "Adding Classifier: Pipeline(steps=[('filter', SelectKBest(k=50, score_func=<function f_classif at 0x109fe80c8>)), ('svc', SVC(C=0.001, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.03125, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "Adding Classifier: Pipeline(steps=[('filter', SelectKBest(k=50, score_func=<function f_classif at 0x109fe80c8>)), ('svc', SVC(C=1e-07, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.5, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "Adding Classifier: XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8)\n"
     ]
    }
   ],
   "source": [
    "ens = []\n",
    "for m in models_dict_1:\n",
    "    m = m.fit(X, y)\n",
    "    print 'Adding Classifier: %s' %(m)\n",
    "    \n",
    "    ens.extend([m] * models_dict_1[m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = load_test('data/test_2008.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = np.asarray([clf.predict(X_test) for clf in models])\n",
    "y_pred = np.asarray([np.argmax(np.bincount(classes[:,c])) \n",
    "                     for c in range(classes.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_test('predictions/ensemble.csv', y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use models until accuracy stopped increasing (25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
       "                     max_features='auto', max_leaf_nodes=None,\n",
       "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                     presort=False, random_state=97, splitter='best'): 1,\n",
       "         DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=1,\n",
       "                     max_features='log2', max_leaf_nodes=None,\n",
       "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                     presort=False, random_state=31, splitter='best'): 1,\n",
       "         DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=5,\n",
       "                     max_features='log2', max_leaf_nodes=None,\n",
       "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                     presort=False, random_state=97, splitter='best'): 1,\n",
       "         DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                     max_features='sqrt', max_leaf_nodes=None,\n",
       "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                     presort=False, random_state=31, splitter='best'): 1,\n",
       "         DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=1,\n",
       "                     max_features=None, max_leaf_nodes=None,\n",
       "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                     presort=False, random_state=31, splitter='best'): 1,\n",
       "         KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                    metric_params=None, n_jobs=1, n_neighbors=63, p=2,\n",
       "                    weights='uniform'): 1,\n",
       "         KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                    metric_params=None, n_jobs=1, n_neighbors=113, p=2,\n",
       "                    weights='uniform'): 1,\n",
       "         Pipeline(steps=[('filter', SelectKBest(k=50, score_func=<function f_classif at 0x109fe80c8>)), ('svc', SVC(C=9.9999999999999995e-07, cache_size=200, class_weight=None, coef0=0.0,\n",
       "           decision_function_shape=None, degree=3, gamma=0.03125, kernel='rbf',\n",
       "           max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "           tol=0.001, verbose=False))]): 1,\n",
       "         Pipeline(steps=[('filter', SelectKBest(k=50, score_func=<function f_classif at 0x109fe80c8>)), ('svc', SVC(C=1.0000000000000001e-05, cache_size=200, class_weight=None, coef0=0.0,\n",
       "           decision_function_shape=None, degree=3, gamma=0.0625, kernel='rbf',\n",
       "           max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "           tol=0.001, verbose=False))]): 1,\n",
       "         Pipeline(steps=[('filter', SelectKBest(k=50, score_func=<function f_classif at 0x109fe80c8>)), ('svc', SVC(C=0.0001, cache_size=200, class_weight=None, coef0=0.0,\n",
       "           decision_function_shape=None, degree=3, gamma=0.015625, kernel='rbf',\n",
       "           max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "           tol=0.001, verbose=False))]): 1,\n",
       "         Pipeline(steps=[('filter', SelectKBest(k=50, score_func=<function f_classif at 0x109fe80c8>)), ('svc', SVC(C=0.0001, cache_size=200, class_weight=None, coef0=0.0,\n",
       "           decision_function_shape=None, degree=3, gamma=4.0, kernel='rbf',\n",
       "           max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "           tol=0.001, verbose=False))]): 1,\n",
       "         Pipeline(steps=[('filter', SelectKBest(k=50, score_func=<function f_classif at 0x109fe80c8>)), ('svc', SVC(C=9.9999999999999995e-08, cache_size=200, class_weight=None, coef0=0.0,\n",
       "           decision_function_shape=None, degree=3, gamma=0.5, kernel='rbf',\n",
       "           max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "           tol=0.001, verbose=False))]): 1,\n",
       "         RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "                     max_depth=20, max_features='log2', max_leaf_nodes=None,\n",
       "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                     n_estimators=20, n_jobs=1, oob_score=False, random_state=None,\n",
       "                     verbose=0, warm_start=False): 1,\n",
       "         RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "                     max_depth=20, max_features='log2', max_leaf_nodes=None,\n",
       "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                     n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
       "                     verbose=0, warm_start=False): 1,\n",
       "         RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "                     max_depth=5, max_features=None, max_leaf_nodes=None,\n",
       "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                     n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
       "                     verbose=0, warm_start=False): 1,\n",
       "         SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
       "                eta0=0.1, fit_intercept=True, l1_ratio=0.5,\n",
       "                learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
       "                penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
       "                verbose=0, warm_start=False): 1,\n",
       "         SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "                eta0=0.01, fit_intercept=True, l1_ratio=0.0,\n",
       "                learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
       "                penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
       "                verbose=0, warm_start=False): 1,\n",
       "         SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
       "                eta0=0.001, fit_intercept=True, l1_ratio=0.5,\n",
       "                learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
       "                penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
       "                verbose=0, warm_start=False): 1,\n",
       "         SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "                eta0=0.001, fit_intercept=True, l1_ratio=0.0,\n",
       "                learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
       "                penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
       "                verbose=0, warm_start=False): 1,\n",
       "         SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
       "                eta0=0.001, fit_intercept=True, l1_ratio=0.5,\n",
       "                learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
       "                penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
       "                verbose=0, warm_start=False): 1,\n",
       "         SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "                eta0=0.001, fit_intercept=True, l1_ratio=1.0,\n",
       "                learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
       "                penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
       "                verbose=0, warm_start=False): 1,\n",
       "         SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
       "                eta0=0.1, fit_intercept=True, l1_ratio=0.0,\n",
       "                learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
       "                penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
       "                verbose=0, warm_start=False): 1,\n",
       "         XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
       "                gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
       "                min_child_weight=1, missing=nan, n_estimators=200, nthread=-1,\n",
       "                objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "                scale_pos_weight=1, seed=0, silent=True, subsample=0.9): 1,\n",
       "         XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
       "                gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
       "                min_child_weight=1, missing=nan, n_estimators=100, nthread=-1,\n",
       "                objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "                scale_pos_weight=1, seed=0, silent=True, subsample=0.5): 1,\n",
       "         XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
       "                gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
       "                min_child_weight=1, missing=nan, n_estimators=100, nthread=-1,\n",
       "                objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "                scale_pos_weight=1, seed=0, silent=True, subsample=0.9): 1})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(selected_models[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models_dict_1 = {DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
    "                     max_features='auto', max_leaf_nodes=None,\n",
    "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                     presort=False, random_state=97, splitter='best'): 1,\n",
    "         DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=1,\n",
    "                     max_features='log2', max_leaf_nodes=None,\n",
    "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                     presort=False, random_state=31, splitter='best'): 1,\n",
    "         DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=5,\n",
    "                     max_features='log2', max_leaf_nodes=None,\n",
    "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                     presort=False, random_state=97, splitter='best'): 1,\n",
    "         DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "                     max_features='sqrt', max_leaf_nodes=None,\n",
    "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                     presort=False, random_state=31, splitter='best'): 1,\n",
    "         DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=1,\n",
    "                     max_features=None, max_leaf_nodes=None,\n",
    "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                     presort=False, random_state=31, splitter='best'): 1,\n",
    "         KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "                    metric_params=None, n_jobs=1, n_neighbors=63, p=2,\n",
    "                    weights='uniform'): 1,\n",
    "         KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "                    metric_params=None, n_jobs=1, n_neighbors=113, p=2,\n",
    "                    weights='uniform'): 1,\n",
    "         Pipeline(steps=[('filter', SelectKBest(k=50, score_func=f_classif)), ('svc', SVC(C=9.9999999999999995e-07, cache_size=200, class_weight=None, coef0=0.0,\n",
    "           decision_function_shape=None, degree=3, gamma=0.03125, kernel='rbf',\n",
    "           max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
    "           tol=0.001, verbose=False))]): 1,\n",
    "         Pipeline(steps=[('filter', SelectKBest(k=50, score_func=f_classif)), ('svc', SVC(C=1.0000000000000001e-05, cache_size=200, class_weight=None, coef0=0.0,\n",
    "           decision_function_shape=None, degree=3, gamma=0.0625, kernel='rbf',\n",
    "           max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
    "           tol=0.001, verbose=False))]): 1,\n",
    "         Pipeline(steps=[('filter', SelectKBest(k=50, score_func=f_classif)), ('svc', SVC(C=0.0001, cache_size=200, class_weight=None, coef0=0.0,\n",
    "           decision_function_shape=None, degree=3, gamma=0.015625, kernel='rbf',\n",
    "           max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
    "           tol=0.001, verbose=False))]): 1,\n",
    "         Pipeline(steps=[('filter', SelectKBest(k=50, score_func=f_classif)), ('svc', SVC(C=0.0001, cache_size=200, class_weight=None, coef0=0.0,\n",
    "           decision_function_shape=None, degree=3, gamma=4.0, kernel='rbf',\n",
    "           max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
    "           tol=0.001, verbose=False))]): 1,\n",
    "         Pipeline(steps=[('filter', SelectKBest(k=50, score_func=f_classif)), ('svc', SVC(C=9.9999999999999995e-08, cache_size=200, class_weight=None, coef0=0.0,\n",
    "           decision_function_shape=None, degree=3, gamma=0.5, kernel='rbf',\n",
    "           max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
    "           tol=0.001, verbose=False))]): 1,\n",
    "         RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
    "                     max_depth=20, max_features='log2', max_leaf_nodes=None,\n",
    "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                     n_estimators=20, n_jobs=1, oob_score=False, random_state=None,\n",
    "                     verbose=0, warm_start=False): 1,\n",
    "         RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
    "                     max_depth=20, max_features='log2', max_leaf_nodes=None,\n",
    "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                     n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
    "                     verbose=0, warm_start=False): 1,\n",
    "         RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
    "                     max_depth=5, max_features=None, max_leaf_nodes=None,\n",
    "                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                     n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
    "                     verbose=0, warm_start=False): 1,\n",
    "         SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
    "                eta0=0.1, fit_intercept=True, l1_ratio=0.5,\n",
    "                learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
    "                penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
    "                verbose=0, warm_start=False): 1,\n",
    "         SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
    "                eta0=0.01, fit_intercept=True, l1_ratio=0.0,\n",
    "                learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
    "                penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
    "                verbose=0, warm_start=False): 1,\n",
    "         SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
    "                eta0=0.001, fit_intercept=True, l1_ratio=0.5,\n",
    "                learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
    "                penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
    "                verbose=0, warm_start=False): 1,\n",
    "         SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
    "                eta0=0.001, fit_intercept=True, l1_ratio=0.0,\n",
    "                learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
    "                penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
    "                verbose=0, warm_start=False): 1,\n",
    "         SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
    "                eta0=0.001, fit_intercept=True, l1_ratio=0.5,\n",
    "                learning_rate='constant', loss='modified_huber', n_iter=5, n_jobs=1,\n",
    "                penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
    "                verbose=0, warm_start=False): 1,\n",
    "         SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
    "                eta0=0.001, fit_intercept=True, l1_ratio=1.0,\n",
    "                learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
    "                penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
    "                verbose=0, warm_start=False): 1,\n",
    "         SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
    "                eta0=0.1, fit_intercept=True, l1_ratio=0.0,\n",
    "                learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
    "                penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
    "                verbose=0, warm_start=False): 1,\n",
    "         XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
    "                gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
    "                min_child_weight=1, n_estimators=200, nthread=-1,\n",
    "                objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
    "                scale_pos_weight=1, seed=0, silent=True, subsample=0.9): 1,\n",
    "         XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
    "                gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
    "                min_child_weight=1, n_estimators=100, nthread=-1,\n",
    "                objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
    "                scale_pos_weight=1, seed=0, silent=True, subsample=0.5): 1,\n",
    "         XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
    "                gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
    "                min_child_weight=1, n_estimators=100, nthread=-1,\n",
    "                objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
    "                scale_pos_weight=1, seed=0, silent=True, subsample=0.9): 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models_list = list(models_dict_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = zip(range(len(models_list)), models_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vclf = VotingClassifier(estimators=m, voting='soft', n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[(0, Pipeline(steps=[('filter', SelectKBest(k=50, score_func=<function f_classif at 0x109fe80c8>)), ('svc', SVC(C=1e-07, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma=0.5, kernel='rbf',\n",
       "  max_iter=-1, probability=True, random_state=None, shr...  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))]))],\n",
       "         n_jobs=4, voting='soft', weights=None)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vclf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = vclf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_test('predictions/ensemble2.csv', y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models (50) selected using the log loss as a measure for choosing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models_dict_2 = {\n",
    "         KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "                    n_neighbors=1, p=2, weights='uniform'): 10,\n",
    "        KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "                    n_neighbors=10, p=2, weights='uniform'): 3,\n",
    "         Pipeline(steps=[('filter', SelectKBest(k=50, score_func=f_classif)), \n",
    "                         ('svc', SVC(C=1.0, cache_size=2000, \n",
    "                                    degree=3, gamma=2.0, kernel='rbf'))]): 7,\n",
    "         Pipeline(steps=[('filter', SelectKBest(k=50, score_func=f_classif)), \n",
    "                         ('svc', SVC(C=1.0, cache_size=2000, \n",
    "                                    degree=3, gamma=4.0, kernel='rbf'))]): 1,\n",
    "         RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
    "                     max_depth=25, max_features='auto', \n",
    "                     n_estimators=100, n_jobs=2, verbose=2): 2,\n",
    "         RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
    "                     max_depth=25, max_features=None,\n",
    "                     n_estimators=100, n_jobs=2, verbose=2): 14,\n",
    "         RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
    "                     max_depth=25, max_features=None, \n",
    "                     n_estimators=50, n_jobs=2, verbose=2): 6,\n",
    "         RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                     max_depth=25, max_features=None,\n",
    "                     n_estimators=20, n_jobs=2, verbose=2): 1,\n",
    "         RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                     max_depth=25, max_features='auto',\n",
    "                     n_estimators=100, n_jobs=2, verbose=2): 1,\n",
    "         SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
    "                eta0=0.01, fit_intercept=True, l1_ratio=0.0,\n",
    "                learning_rate='constant', loss='log', n_iter=5, n_jobs=1,\n",
    "                penalty='elasticnet'): 1,\n",
    "         XGBClassifier(base_score=0.5, colsample_bytree=0.9,\n",
    "                   gamma=0, learning_rate=0.1, max_depth=10, \n",
    "                       n_estimators=300, subsample=0.5): 4\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
